---
layout: post
published: true
category: commentary
title: Assignment 18
author: Xiaofan Ye
---
1.In your comments on GitHub, discuss how these text-mining approaches have become a standard in today's computa1onal techniques in processing texts and the potential (and the shortcomings) that was expected but not necessarily been realized 
or superseded by novel/different approaches. 

The three texts introduced the field of text data mining and how it could be implemented in real life. Text Data Mining is helpful in many fields, such as in bioscience, where scientists can extract patterns and deduct hypotheses about the disease from a combination of text fragments in biomedical literature titles. Moreover, text data mining could help us uncover social impact. As we can see from our classmate Daniel's group's approach-- deriving trends in social attitudes about Russia/Ukrain issue by analyzing textual patterns from Twitter memes.

However, text data mining also has its own limitations: it couldn't represent all the information and knowledge beyond the communication of texts. For example, many types of knowledge are conveyed through other media like graphics and sound. Text data mining couldn't translate those forms of media, therefore, it might cause misunderstanding in interpreting specific topics that can only be fully comprehended through those media. Furthermore, text data mining couldn't uncover the decisions and thought processes behind the door; the textual data is just a lower-level data identification and organization. It still depends heavily on human cognition to link different sources and form hypotheses based on the processes of textual data.


2.Read Lisa Gitelman's (ed.) "Raw Data" Is an Oxymoron, Introduc1on (Cambridge, Mass, MIT-Press, 2013, 1-14) and comment on the often used concept of "raw data." (text on Canvas)


There's never anything about 'raw' data. All the data we are exposed to have already been 'cooked' -- they have been processed and interpreted by different professionals in different disciplines through their own methods. All the data are not isolated; they are all contextualized and stored within a social-cultural context. For example, the language that records the data already contains the cultural construct that adds one more layer to the 'original' data.

The article also mentions the term data aggregation -- the overlay of operations that renders data's value also has impacts. The steps of different levels of obtaining the data sets, from data gathering to cleaning to analyzing, are all embedded with the researchers' interpretations and assumptions and are relatively biased. For example, changing one filter in the algorithm model would produce different results. Maintaining the consistency of research methods to get stable data is crucial.


